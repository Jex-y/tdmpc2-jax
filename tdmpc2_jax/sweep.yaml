program: train.py
method: bayes
metric: 
  goal: maximize
  name: cumulative_reward
parameters:
  hardcore:
    value: true
  no_improvement_window:
    value: 500
  max_episodes:
    value: 500
  seed_steps: 
    distribution: uniform
    min: 1000
    max: 100_000
  encoder.encoder_dim:
    values: [128, 256, 512]
  encoder.num_encoder_layers:
    values: [1, 2]
  world_model.mlp_dim:
    values: [128, 256, 512]
  world_model.latent_dim:
    values: [128, 256, 512]
  world_model.value_dropout:
    values: [0, 0.001, 0.005, 0.01]
  world_model.num_value_nets:
    values: [2, 5]
  world_model.symlog_min:
    values: [-10, -5]
  world_model.symlog_max:
    values: [5, 10]
  world_model.learning_rate:
    distribution: log_uniform_values
    min: 1e-4
    max: 1e-3
  world_model.encoder_learning_rate:
    distribution: log_uniform_values
    min: 5e-5
    max: 1e-3
  tdmpc2.horizon:
    values: [1, 3, 5, 8]
  tdmpc2.mppi_iterations:
    values: [4, 6, 8, 10]
  tdmpc2.population_size:
    values: [256, 512, 1024, 2048]
  tdmpc2.temperature:
    values: [0.1, 0.5, 1]
command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}
