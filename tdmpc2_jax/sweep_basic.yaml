program: train.py
method: bayes
metric: 
  goal: maximize
  name: cumulative_reward
parameters:
  hardcore:
    value: false
  no_improvement_window:
    value: 0
  max_episodes:
    value: 100
  seed_steps: 
    distribution: q_uniform
    q: 100
    min: 2000
    max: 20_000
  seed_update_ratio:
    values: [0, 0.25, 0.5, 0.8, 1]
  encoder.encoder_dim:
    values: [32, 40, 48, 56, 64, 72, 80]
  encoder.num_encoder_layers:
    value: 2
  world_model.mlp_dim:
    values: [32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160]
  world_model.latent_dim:
    values: [32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256]
  world_model.value_dropout:
    value: 0.005
  world_model.num_value_nets:
    value: 5
  world_model.learning_rate:
    distribution: q_log_uniform_values
    q: 1e-4
    min: 1e-4
    max: 1e-3
  world_model.encoder_learning_rate:
    distribution: q_log_uniform_values
    q: 1e-4
    min: 1e-4
    max: 1e-3
  world_model.predict_continues:
    value: true
  tdmpc2.horizon:
    value: 5
  tdmpc2.mppi_iterations:
    value: 6
  tdmpc2.population_size:
    value: 1024
  tdmpc2.temperature:
    distribution: q_uniform
    q: 0.01
    min: 0.05
    max: 0.8
  tdmpc2.entropy_coef:
    distribution: q_log_uniform_values
    q: 1e-5
    min: 1e-5
    max: 1e-4
command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}
